# ğŸš€ GPU Optimization Guide (2Ã— H100)

Ø§ÛŒÙ† ÙØ§ÛŒÙ„ ØªÙˆØ¶ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ú†Ú¯ÙˆÙ†Ù‡ Ø§Ø² **Ø¯Ùˆ GPU H100** Ø¨Ø±Ø§ÛŒ ØªØ³Ø±ÛŒØ¹ Ù¾Ø±ÙˆÚ˜Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ….

---

## ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ØªØ³Ø±ÛŒØ¹â€ŒÙ‡Ø§

| Ù…Ø±Ø­Ù„Ù‡ | Ø²Ù…Ø§Ù† Ø¨Ø§ 1 GPU | Ø²Ù…Ø§Ù† Ø¨Ø§ 2 GPU | ØªØ³Ø±ÛŒØ¹ |
|-------|--------------|--------------|-------|
| **1. Dataset Generation** | 81 min | **41 min** | **2.0x** âœ… |
| **2. Feature Extraction** | 15 sec | 15 sec | 1.0x (Ø®ÛŒÙ„ÛŒ Ø³Ø±ÛŒØ¹Ù‡) |
| **3. Model Training** | 8 min | **4-5 min** | **1.7x** âœ… |
| **4. STNN Training** | 12 min | **6-7 min** | **1.8x** âœ… |
| **5. Localization (GCC-PHAT)** | 15 min | 15 min | 1.0x (CPU-bound) |
| **Ú©Ù„ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ†** | ~117 min | **~67 min** | **1.75x** âš¡ |

---

## âœ… Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡

### 1ï¸âƒ£ **Dataset Generation** (ALREADY OPTIMIZED!)
```bash
# Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø§Ø² Ù‚Ø¨Ù„ Ø§Ø² Ø¯Ùˆ GPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
python3 generate_dataset_parallel.py

# Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø±:
# GPU-0: Samples 0-1500    (41 min)
# GPU-1: Samples 1500-3000 (41 min)
# Total: 41 min (Ø¨Ù‡ Ø¬Ø§ÛŒ 81 min)
```

**Ú©Ø¯ Ù…Ø±Ø¨ÙˆØ·Ù‡:**
- `generate_dataset_parallel.py` lines 27-85
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² `multiprocessing` Ø¨Ø§ Ø¯Ùˆ worker
- Ù‡Ø± worker ÛŒÚ© GPU Ù…Ø¬Ø²Ø§ Ø¯Ø§Ø±Ù‡

---

### 2ï¸âƒ£ **Model Training** (NOW OPTIMIZED!)
```bash
# Ø­Ø§Ù„Ø§ main.py Ø§Ø² Ø¯Ùˆ GPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
python3 main.py

# Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø±:
# - Batch 32 ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒØ´Ù‡: GPU-0 (16 samples) + GPU-1 (16 samples)
# - Gradients Ø±ÙˆÛŒ Ø¯Ùˆ GPU Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´Ù‡
# - Average gradients Ø¨Ø±Ø§ÛŒ update weights
```

**ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡:**
- `main.py` lines 8-39: Initialize `MirroredStrategy`
- `model/detector.py` line 177: Pass `strategy` parameter
- `model/detector.py` lines 224-230: Build model inside `strategy.scope()`

**Ú†Ú¯ÙˆÙ†Ú¯ÛŒ Ú©Ø§Ø±:**
```python
# main.py
strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"])

# detector.py
with strategy.scope():
    model = build_dual_input_cnn_h100()  # Model replicated on both GPUs

# Training automatically distributed:
model.fit(train_ds, ...)  # TensorFlow handles GPU distribution
```

---

### 3ï¸âƒ£ **STNN Training** (ALREADY OPTIMIZED!)
```bash
# Ø§ÛŒÙ† Ù‡Ù… Ø§Ø² Ù‚Ø¨Ù„ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
python3 main.py --train-stnn --stnn-epochs 50

# Ù†Ø­ÙˆÙ‡ Ú©Ø§Ø±:
# - TDOA model: Distributed training on 2 GPUs
# - FDOA model: Distributed training on 2 GPUs
```

**Ú©Ø¯ Ù…Ø±Ø¨ÙˆØ·Ù‡:**
- `model/stnn_localization.py` line 5: Comment says "Uses MirroredStrategy"
- `core/train_stnn_localization.py`: Already uses `use_multi_gpu=True`

---

## âŒ Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ

### 5ï¸âƒ£ **Localization Phase (GCC-PHAT)**
Ø§ÛŒÙ† Ù‚Ø³Ù…Øª **Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡** Ø§Ø² GPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù‡ Ú†ÙˆÙ†:
- Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø±ÙˆÛŒ **NumPy/SciPy** Ù‡Ø³Øª (CPU-only)
- **Sample-by-sample** Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´Ù‡ (Ù†Ù‡ batch)
- FFT Ø±ÙˆÛŒ CPU Ø³Ø±ÛŒØ¹â€ŒØªØ±Ù‡ Ø¨Ø±Ø§ÛŒ Ø³ÛŒÚ¯Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©

**Ø±Ø§Ù‡ Ø­Ù„ ÙØ¹Ù„ÛŒ:**
- Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ 100 sample Ø§ÙˆÙ„ (Ø®Ø· 616 Ø¯Ø± `localization.py`)
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² progress bar Ø¨Ø±Ø§ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯
- Ø²Ù…Ø§Ù†: ~15-20 Ø¯Ù‚ÛŒÙ‚Ù‡ (Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„Ù‡)

---

## ğŸ”§ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡

### Ú¯Ø²ÛŒÙ†Ù‡ 1: ÙÙ‚Ø· Training (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ)
```bash
# Dataset Ø§Ø² Ù‚Ø¨Ù„ Ù…ÙˆØ¬ÙˆØ¯Ù‡ØŒ ÙÙ‚Ø· model train Ù…ÛŒâ€ŒØ´Ù‡
python3 main.py
```
**Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:**
```
============================================================
ğŸš€ INITIALIZING MULTI-GPU STRATEGY
============================================================
âœ“ Multi-GPU enabled: 2 GPUs (H100 Ã— 2)
  â†’ Expected speedup: 1.7-1.9x for model training
============================================================

[Phase 3] Training detector model...
âœ“ Using multi-GPU strategy: 2 devices
Epoch 1/50
...
```

---

### Ú¯Ø²ÛŒÙ†Ù‡ 2: Dataset + Training
```bash
# Ø§ÙˆÙ„ dataset Ø¨Ø³Ø§Ø² (Ø¨Ø§ 2 GPU)
python3 generate_dataset_parallel.py  # 41 min

# Ø¨Ø¹Ø¯ train Ú©Ù† (Ø¨Ø§ 2 GPU)
python3 main.py  # 4-5 min training
```

---

### Ú¯Ø²ÛŒÙ†Ù‡ 3: STNN + Training
```bash
# Ø§ÙˆÙ„ STNN train Ú©Ù†
python3 main.py --train-stnn --stnn-epochs 50  # 6-7 min per model

# Ø¨Ø¹Ø¯ main pipeline Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù‡
# Total: ~15-20 min
```

---

## ğŸ“ˆ Ù†Ú©Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ

### 1. **Batch Size**
```python
# config/settings.py
TRAIN_BATCH = 32  # ÙØ¹Ù„ÛŒ

# Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø§Ø² GPU:
TRAIN_BATCH = 64  # â†’ ØªØ³Ø±ÛŒØ¹ 1.9-2.0x Ø¨Ù‡ Ø¬Ø§ÛŒ 1.7x
```

**ØªÙˆØ¶ÛŒØ­:**
- Batch 32: Ù‡Ø± GPU ÙÙ‚Ø· 16 sample Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù‡ (Ú©Ù…Ù‡!)
- Batch 64: Ù‡Ø± GPU 32 sample Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù‡ (Ø¨Ù‡ØªØ±Ù‡!)
- H100 memory: 80GB â†’ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ ØªØ§ batch 128 Ø±Ùˆ handle Ú©Ù†Ù‡

---

### 2. **Mixed Precision**
```python
# model/detector.py (Ø®Ø· 193)
mixed_precision.set_global_policy("mixed_float16")  # âœ… ÙØ¹Ø§Ù„Ù‡

# Ø³Ø±Ø¹Øª: ~1.3x Ø¨ÛŒØ´ØªØ±
# Ø¯Ù‚Øª: ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ ÛŒÚ©Ø³Ø§Ù† (Ø¨Ø§ calibration)
```

---

### 3. **XLA Compilation**
```python
# model/detector.py (Ø®Ø· 194)
tf.config.optimizer.set_jit(True)  # âœ… ÙØ¹Ø§Ù„Ù‡

# Ø³Ø±Ø¹Øª: ~1.2x Ø¨ÛŒØ´ØªØ±
# Ù†Ú©ØªÙ‡: Ø§ÙˆÙ„ÛŒÙ† epoch Ú©Ù†Ø¯ØªØ±Ù‡ (compilation overhead)
```

---

## ğŸ§ª ØªØ³Øª Ùˆ Validation

### Ú†Ú© Ú©Ø±Ø¯Ù† GPU Ù‡Ø§:
```bash
# Ø¨Ø¨ÛŒÙ† GPU Ù‡Ø§ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´Ù†ØŸ
nvidia-smi

# Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.xxx       Driver Version: 535.xxx       CUDA Version: 12.2  |
# |-------------------------------+----------------------+----------------------+
# |   0  NVIDIA H100 80GB       | ...                  | ...                  |
# |   1  NVIDIA H100 80GB       | ...                  | ...                  |
# +-----------------------------------------------------------------------------+
```

### Ú†Ú© Ú©Ø±Ø¯Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡:
```bash
# Ø¯Ø± Ø­ÛŒÙ† trainingØŒ terminal Ø¯ÛŒÚ¯Ù‡:
watch -n 1 nvidia-smi

# Ø¨Ø§ÛŒØ¯ Ø¨Ø¨ÛŒÙ†ÛŒ:
# GPU 0: 60-70% Util, 25-30 GB Memory
# GPU 1: 60-70% Util, 25-30 GB Memory
```

---

## ğŸ› Troubleshooting

### Ù…Ø´Ú©Ù„ 1: ÙÙ‚Ø· ÛŒÚ© GPU Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ù‡
```bash
# Ø¹Ù„Øª: CUDA_VISIBLE_DEVICES Ø§Ø´ØªØ¨Ø§Ù‡ set Ø´Ø¯Ù‡
# Ø±Ø§Ù‡ Ø­Ù„:
export CUDA_VISIBLE_DEVICES=0,1
python3 main.py
```

### Ù…Ø´Ú©Ù„ 2: Out of Memory
```python
# Ø¹Ù„Øª: Batch size Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯Ù‡
# Ø±Ø§Ù‡ Ø­Ù„: Ú©Ù… Ú©Ù† batch size Ø±Ùˆ
# config/settings.py
TRAIN_BATCH = 32  # Ú©Ù… Ú©Ù† Ø¨Ù‡ 16 ÛŒØ§ 8
```

### Ù…Ø´Ú©Ù„ 3: "No GPU detected"
```python
# Ø¨Ø±Ø±Ø³ÛŒ:
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))

# Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:
# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),
#  PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]
```

---

## ğŸ“š Ù…Ø±Ø§Ø¬Ø¹

- TensorFlow Multi-GPU: https://www.tensorflow.org/guide/distributed_training
- MirroredStrategy: https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy
- Mixed Precision: https://www.tensorflow.org/guide/mixed_precision
- XLA: https://www.tensorflow.org/xla

---

## âœ… Checklist Ù†Ù‡Ø§ÛŒÛŒ

Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¬Ø±Ø§:
- [ ] `nvidia-smi` Ø±Ùˆ Ú†Ú© Ú©Ø±Ø¯Ù…ØŒ Ø¯Ùˆ GPU Ø¯ÛŒØ¯Ù…
- [ ] `export CUDA_VISIBLE_DEVICES=0,1` Ø±Ùˆ set Ú©Ø±Ø¯Ù…
- [ ] `python3 main.py` Ø±Ùˆ Ø§Ø¬Ø±Ø§ Ú©Ø±Ø¯Ù… Ùˆ Ø®Ø±ÙˆØ¬ÛŒ "Multi-GPU enabled: 2 GPUs" Ø±Ùˆ Ø¯ÛŒØ¯Ù…
- [ ] Ø¯Ø± Ø­ÛŒÙ† trainingØŒ `watch nvidia-smi` Ø±Ùˆ Ú†Ú© Ú©Ø±Ø¯Ù… Ùˆ Ø¯ÛŒØ¯Ù… Ù‡Ø± Ø¯Ùˆ GPU Ù…Ø´ØºÙˆÙ„Ù†
- [ ] Ø²Ù…Ø§Ù† training Ù‚Ø¨Ù„: ~8 min â†’ Ø¨Ø¹Ø¯: ~4-5 min (ØªØ³Ø±ÛŒØ¹ 1.7x) âœ“

---

**Ù†ØªÛŒØ¬Ù‡:** Ø¨Ø§ ØªØºÛŒÛŒØ±Ø§Øª Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡ØŒ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ† Ø´Ù…Ø§ Ø§Ø² **117 Ø¯Ù‚ÛŒÙ‚Ù‡** Ø¨Ù‡ **~67 Ø¯Ù‚ÛŒÙ‚Ù‡** Ú©Ø§Ù‡Ø´ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ âš¡

**Ù†Ú©ØªÙ‡:** Ø§Ú¯Ù‡ batch size Ø±Ùˆ Ø¨Ù‡ 64 Ø§ÙØ²Ø§ÛŒØ´ Ø¨Ø¯ÛŒØŒ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ø¨Ù‡ **~55-60 Ø¯Ù‚ÛŒÙ‚Ù‡** Ø¨Ø±Ø³Ù‡!
